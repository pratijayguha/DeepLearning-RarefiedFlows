{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Softmax, Multiply, Lambda, Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def make_dir(path): # Function to make adirectory to save files in\n",
    "    cwd = os.getcwd()\n",
    "    abs_path = cwd + path\n",
    "    count = 1\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(abs_path):\n",
    "        try: \n",
    "            os.makedirs(abs_path)\n",
    "            print(abs_path)\n",
    "        except OSError:return count*0\n",
    "        else: return count\n",
    "    else: return -1*count\n",
    "\n",
    "def get_APE(pred,y_true): # Function to return an array with APE of eaaaah prediction \n",
    "    APE = np.zeros(y_true.shape[0])\n",
    "    for i in range(y_true.shape[0]):\n",
    "        APE[i] = abs((pred[i] - y_true[i]) / y_true[i])\n",
    "\n",
    "    return APE\n",
    "\n",
    "def get_max_APE(APE): # Function to return the maximum APE with index value\n",
    "    max_APE = np.zeros(2)\n",
    "\n",
    "    for i in range(40, 160):\n",
    "        if APE[i] > max_APE[0]:\n",
    "            max_APE[0] = APE[i]\n",
    "            max_APE[1] = i\n",
    "    return max_APE\n",
    "\n",
    "# Declaring Variables\n",
    "num_epochs = 2000000\n",
    "\n",
    "\n",
    "# Filepaths for saving Model and PostProcessed data:\n",
    "save_post_dir = 'PostProc//SNwithoutMult/OptiStudies/'\n",
    "save_model_dir = 'SavedModels/SNwithoutMult/OptiStudies/'\n",
    "\n",
    "# Reading data from .csv file\n",
    "data_dat = pd.read_csv (r'data.csv')\n",
    "pred_dat = pd.read_csv (r'test.csv')\n",
    "\n",
    "# Extracting data into numpy arrays from pandas DataFrames\n",
    "\n",
    "X = pd.DataFrame(data_dat, columns = ['LD'])\n",
    "X = X.to_numpy()\n",
    "\n",
    "X_pred = pd.DataFrame(pred_dat, columns = ['LD'])\n",
    "X_pred = X_pred.to_numpy()\n",
    "X_pred_ns = X_pred\n",
    "\n",
    "y = pd.DataFrame(data_dat, columns = ['Ma'])\n",
    "y = y.to_numpy()\n",
    "\n",
    "y_pred = pd.DataFrame(pred_dat, columns = ['Ma'])\n",
    "y_pred = y_pred.to_numpy()\n",
    "\n",
    "X_n = pd.DataFrame(data_dat, columns = ['XL'])\n",
    "X_n = X_n.to_numpy()\n",
    "\n",
    "X_n_pred = pd.DataFrame(pred_dat, columns = ['XL'])\n",
    "X_n_pred = X_n_pred.to_numpy()\n",
    "\n",
    "\n",
    "\n",
    "def get_norm_y(y,c): # Returns normalised valeus of Mach Number and Maximum Mach Number per L/D ratio as two arrays\n",
    "    num = int(y.shape[0] / c)\n",
    "    max_val = np.zeros(c*num)\n",
    "    y_norm = np.zeros([num*c])\n",
    "    for i in range(num):\n",
    "        max_val[i*c:(i+1)*c] = max(y[i*c:(i+1)*c])\n",
    "        for j in range(c):\n",
    "            y_norm[i*c + j] = y[i*c+j]/max_val[i*c+j]\n",
    "    return y_norm, max_val\n",
    "    \n",
    "y_norm, max_y = get_norm_y(y, 201)\n",
    "y_pred_norm, max_y_pred = get_norm_y(y_pred, 201)\n",
    "\n",
    "def get_x(x,c): # Returns Input set for training the Max Mach Numbers\n",
    "    x_n = np.zeros(x.shape[0])\n",
    "    \n",
    "    for i in range(x.shape[0]):\n",
    "        x_n[i] = x[i]\n",
    "    \n",
    "    return x_n\n",
    "\n",
    "X_max = get_x(X,201)\n",
    "X_pred_max = get_x(X_pred, 201)\n",
    "\n",
    "X= np.hstack((X_n,X))\n",
    "X_pred = np.hstack((X_n_pred,X_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a concatenated output file.\n",
    "y_comb = np.zeros([y_norm.shape[0],2])\n",
    "y_comb[:,0]=y_norm\n",
    "y_comb[:,1]=max_y\n",
    "\n",
    "y_pred_comb = np.zeros([y_pred_norm.shape[0],2])\n",
    "y_pred_comb[:,0]=y_pred_norm\n",
    "y_pred_comb[:,1]=max_y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists\n",
      "Directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Using sklearn to split the data into train and test sets with shuffling ####\n",
    "X_train, X_test, y_train_comb, y_test_comb = train_test_split(X, y_comb, test_size=0.2, random_state=42, shuffle=True)\n",
    "\n",
    "#### Creating directory to save outputs ####\n",
    "val_makedir = make_dir('/' + save_post_dir)\n",
    "if val_makedir == 1:\n",
    "    print('Directory has been created')\n",
    "elif val_makedir == -1:\n",
    "    print('Directory already exists')\n",
    "else:\n",
    "    print('Failed to create directory')\n",
    "    sys.exit(0)\n",
    "\n",
    "val_makedir = make_dir('/' + save_model_dir)\n",
    "if val_makedir == 1:\n",
    "    print('Directory has been created')\n",
    "elif val_makedir == -1:\n",
    "    print('Directory already exists')\n",
    "else:\n",
    "    print('Failed to create directory')\n",
    "    sys.exit(0)\n",
    "\n",
    "\n",
    "def load_optimizers(): # Function to load all optimizers\n",
    "    Adadelta = tf.keras.optimizers.Adadelta(learning_rate=0.001,\n",
    "                                            rho=0.95, epsilon=1e-07)\n",
    "    RMSprop = tf.keras.optimizers.RMSprop(learning_rate=0.001, rho=0.9)\n",
    "    Adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.01)\n",
    "    Adam = tf.keras.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    SGD_NM = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.5, nesterov=True)\n",
    "    SGD = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    Nadam = tf.keras.optimizers.Nadam(learning_rate=0.002, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "    opti_list = [RMSprop, Adadelta, Adagrad, SGD_NM, SGD, Adam, Nadam]\n",
    "    opti_name_list = [\"RMSprop\", \"Adadelta\", \"Adagrad\", \"SGD_NM\", \"SGD\", \"Adam\", \"Nadam\"]\n",
    "    return opti_list, opti_name_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 482 samples, validate on 121 samples\n",
      "Epoch 1/2\n",
      "482/482 [==============================] - 3s 5ms/sample - loss: 8.3750 - IndVal_Final_layer_loss: 0.3398 - MaxVal_Final_layer_loss: 7.8089 - IndVal_Final_layer_mape: 93.0284 - MaxVal_Final_layer_mape: 92.9887 - val_loss: 7.4155 - val_IndVal_Final_layer_loss: 0.1204 - val_MaxVal_Final_layer_loss: 7.2806 - val_IndVal_Final_layer_mape: 171.5115 - val_MaxVal_Final_layer_mape: 87.6300\n",
      "Epoch 2/2\n",
      "482/482 [==============================] - 0s 574us/sample - loss: 6.5711 - IndVal_Final_layer_loss: 0.1280 - MaxVal_Final_layer_loss: 6.3883 - IndVal_Final_layer_mape: 199.2654 - MaxVal_Final_layer_mape: 81.5829 - val_loss: 5.4398 - val_IndVal_Final_layer_loss: 0.1151 - val_MaxVal_Final_layer_loss: 5.3137 - val_IndVal_Final_layer_mape: 181.1912 - val_MaxVal_Final_layer_mape: 72.5290\n",
      "INFO:tensorflow:Assets written to: SavedModels/SNwithoutMult/OptiStudies/model_RMSprop/assets\n",
      "Train on 482 samples, validate on 121 samples\n",
      "Epoch 1/2\n",
      "482/482 [==============================] - 3s 6ms/sample - loss: 9.3852 - IndVal_Final_layer_loss: 0.3382 - MaxVal_Final_layer_loss: 8.8388 - IndVal_Final_layer_mape: 92.5745 - MaxVal_Final_layer_mape: 99.7097 - val_loss: 9.1751 - val_IndVal_Final_layer_loss: 0.1228 - val_MaxVal_Final_layer_loss: 9.0350 - val_IndVal_Final_layer_mape: 164.9383 - val_MaxVal_Final_layer_mape: 99.3574\n",
      "Epoch 2/2\n",
      "482/482 [==============================] - 0s 554us/sample - loss: 9.0756 - IndVal_Final_layer_loss: 0.1266 - MaxVal_Final_layer_loss: 8.7507 - IndVal_Final_layer_mape: 206.5158 - MaxVal_Final_layer_mape: 99.0516 - val_loss: 9.0699 - val_IndVal_Final_layer_loss: 0.1083 - val_MaxVal_Final_layer_loss: 8.9450 - val_IndVal_Final_layer_mape: 206.9755 - val_MaxVal_Final_layer_mape: 98.7145\n",
      "INFO:tensorflow:Assets written to: SavedModels/SNwithoutMult/OptiStudies/model_Adadelta/assets\n",
      "Train on 482 samples, validate on 121 samples\n",
      "Epoch 1/2\n",
      "482/482 [==============================] - 4s 7ms/sample - loss: 9.2433 - IndVal_Final_layer_loss: 0.2760 - MaxVal_Final_layer_loss: 8.9453 - IndVal_Final_layer_mape: 98.2961 - MaxVal_Final_layer_mape: 99.1375 - val_loss: 8.9698 - val_IndVal_Final_layer_loss: 0.1147 - val_MaxVal_Final_layer_loss: 8.8392 - val_IndVal_Final_layer_mape: 230.6488 - val_MaxVal_Final_layer_mape: 98.0235\n",
      "Epoch 2/2\n",
      "482/482 [==============================] - 0s 725us/sample - loss: 8.7630 - IndVal_Final_layer_loss: 0.1216 - MaxVal_Final_layer_loss: 8.2577 - IndVal_Final_layer_mape: 213.5740 - MaxVal_Final_layer_mape: 96.9354 - val_loss: 8.5930 - val_IndVal_Final_layer_loss: 0.1095 - val_MaxVal_Final_layer_loss: 8.4677 - val_IndVal_Final_layer_mape: 196.7020 - val_MaxVal_Final_layer_mape: 95.4175\n",
      "INFO:tensorflow:Assets written to: SavedModels/SNwithoutMult/OptiStudies/model_Adagrad/assets\n",
      "Train on 482 samples, validate on 121 samples\n",
      "Epoch 1/2\n",
      "482/482 [==============================] - 3s 5ms/sample - loss: 9.5212 - IndVal_Final_layer_loss: 0.4653 - MaxVal_Final_layer_loss: 9.2303 - IndVal_Final_layer_mape: 101.9184 - MaxVal_Final_layer_mape: 99.7133 - val_loss: 9.3664 - val_IndVal_Final_layer_loss: 0.3134 - val_MaxVal_Final_layer_loss: 9.0340 - val_IndVal_Final_layer_mape: 83.1911 - val_MaxVal_Final_layer_mape: 99.3504\n",
      "Epoch 2/2\n",
      "482/482 [==============================] - 0s 395us/sample - loss: 9.1703 - IndVal_Final_layer_loss: 0.2316 - MaxVal_Final_layer_loss: 9.1382 - IndVal_Final_layer_mape: 122.9613 - MaxVal_Final_layer_mape: 99.0569 - val_loss: 9.0826 - val_IndVal_Final_layer_loss: 0.1233 - val_MaxVal_Final_layer_loss: 8.9433 - val_IndVal_Final_layer_mape: 247.2138 - val_MaxVal_Final_layer_mape: 98.7024\n",
      "INFO:tensorflow:Assets written to: SavedModels/SNwithoutMult/OptiStudies/model_SGD_NM/assets\n"
     ]
    }
   ],
   "source": [
    "opti_list, opti_name_list = load_optimizers()\n",
    "max_MAPE=np.zeros([len(opti_list), int(2*X_pred.shape[0]/201)])\n",
    "for i in range(len(opti_list)):\n",
    "    opti = opti_list[i]\n",
    "    opti_name = opti_name_list[i]\n",
    "\n",
    "    # Define save directory for individual optimizers\n",
    "    save_model_path = save_model_dir+'model_'+opti_name\n",
    "\n",
    "    # Architecture for the Siamese Network\n",
    "    MaxVal_archi = [5,5,5,5,5,5,5,5,5,5,5,5,1]\n",
    "    IndVal_archi = [20,20,20,20,20,20,20,20,20,20,20,20,1]\n",
    "\n",
    "    def get_LD(x): # Function to separate L/D Ratio for the Lambda Layer.\n",
    "        x_new = x[:,1]\n",
    "        return x_new[:,np.newaxis]\n",
    "\n",
    "    # Define a function to build a SN\n",
    "    def build_model(MaxVal_archi, IndVal_archi):\n",
    "\n",
    "        Input_layer = Input(shape=[2,], name='Input')\n",
    "        # Lambda layer to separately use it as only L/D ratio input for a fork in Network.\n",
    "        MaxVal_input = Lambda(get_LD, name='MaxVal_input', output_shape=(1,))(Input_layer)\n",
    "    \t# Defining the MaxVal fork of SN    \n",
    "        MaxVal_layer = []\n",
    "        for i,node in enumerate(MaxVal_archi):\n",
    "            if i==0: # First layer in MaxVal fork\n",
    "                MaxVal_layer.append(Dense(node, name='MaxVal_layer%d' %(i+1), activation='relu')(MaxVal_input))\n",
    "            elif i==len(MaxVal_archi)-1: # Last layer in MaxVal fork. *Activation must be linear*\n",
    "                MaxVal_layer.append(Dense(node, name='MaxVal_Final_layer', activation='linear')(MaxVal_layer[i-1]))\n",
    "            else: # For intermediate layers\n",
    "                MaxVal_layer.append(Dense(node, name='MaxVal_layer%d' %(i+1), activation='relu')(MaxVal_layer[i-1]))\n",
    "        # Defining the IndVal fork of Neural Network\n",
    "        IndVal_layer = []\n",
    "        for i, node in enumerate(IndVal_archi):\n",
    "            if i==0: # First layer in MaxVal fork\n",
    "                IndVal_layer.append(Dense(node, name='IndVal_layer%d' %(i+1), activation='relu')(Input_layer))\n",
    "            elif i==len(IndVal_archi)-1: # Last layer in MaxVal fork. *Activation must be linear*\n",
    "                IndVal_layer.append(Dense(node, name='IndVal_Final_layer', activation='linear')(IndVal_layer[i-1])) \n",
    "            else:# For intermediate layers\n",
    "                IndVal_layer.append(Dense(node, name='IndVal_layer%d' %(i+1), activation='relu')(IndVal_layer[i-1]))\n",
    "    \t# Multiply both layers to get final output.            \n",
    "        # Multiplication_layer = Multiply(name='Multiplication_layer')([IndVal_layer[len(IndVal_layer)-1], MaxVal_layer[len(MaxVal_layer)-1]])\n",
    "        # Building the model with all connections\n",
    "        model = Model(inputs= [Input_layer], outputs= [IndVal_layer[len(IndVal_layer)-1], MaxVal_layer[len(MaxVal_layer)-1]])\n",
    "        return model\n",
    "    # Calling the model\n",
    "    model = build_model(MaxVal_archi,IndVal_archi)\n",
    "    # A visual flowchart of the model.\n",
    "    keras.utils.plot_model(model, \"Architecture_SN.png\", show_shapes=True)\n",
    "\n",
    "    # Define early stopping callback\n",
    "    early_stopping_callback = EarlyStopping(monitor='val_MaxVal_Final_layer_mape', \n",
    "                                            patience=500,\n",
    "                                            min_delta= 1e-4,\n",
    "                                            restore_best_weights=True,\n",
    "                                            mode='auto',\n",
    "                                            verbose=True)\n",
    "    # Compile the model\n",
    "    model.compile(loss='mse', optimizer='Adam', metrics=['mape'])\n",
    "    # Train the model\n",
    "    history = model.fit(X_train,\n",
    "                        (y_train_comb[:,0], y_train_comb[:,1]),\n",
    "                        batch_size = 32,\n",
    "                        epochs=2,\n",
    "                        verbose=True,\n",
    "                        validation_data=(X_test, (y_test_comb[:,0], y_test_comb[:,1])),\n",
    "                        callbacks=[early_stopping_callback])\n",
    "    # Save the best version of the model.\n",
    "    model.save(save_model_dir+'model_'+opti_name)\n",
    "    # write  model training metrics to a dataframe\n",
    "    model_data = pd.DataFrame(history.history)\n",
    "    model_data.head()\n",
    "    # Subplot for Plotting training metrics\n",
    "    nrows = 2\n",
    "    ncols = 2 \n",
    "    # Initialise the plot\n",
    "    fig1 = plt.figure()\n",
    "    fig, axes = plt.subplots(nrows, ncols)\n",
    "    fig.set_size_inches(16,8)\n",
    "    # Plot loss in [0,0]\n",
    "    loss_plot=model_data.plot(y=\"IndVal_Final_layer_loss\", color='blue', label=\"YNorm\", ax=axes[0,0])\n",
    "    loss_plot=model_data.plot(y=\"MaxVal_Final_layer_loss\", color='green', label=\"YMax\", ax=axes[0,0])\n",
    "    loss_plot.set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "    loss_plot.set_title(\"Loss vs Epochs\")\n",
    "    loss_plot.legend(loc=\"upper right\")\n",
    "    # Plot MAPE in [0,1]\n",
    "    mape_plot=model_data.plot(y=\"IndVal_Final_layer_mape\", color='blue', label=\"YNorm\", ax=axes[0,1])\n",
    "    mape_plot=model_data.plot(y=\"MaxVal_Final_layer_mape\", color='green', label=\"YMax\", ax=axes[0,1])\n",
    "    mape_plot.set(xlabel=\"Epochs\", ylabel=\"MAPE\")\n",
    "    mape_plot.set_title(\"MAPE vs Epochs\")\n",
    "    mape_plot.legend(loc=\"upper right\")\n",
    "    # Plot val_loss in [1,0]\n",
    "    val_loss_plot=model_data.plot(y=\"val_IndVal_Final_layer_loss\", color='blue', label=\"YNorm\", ax=axes[1,0])\n",
    "    val_loss_plot=model_data.plot(y=\"val_MaxVal_Final_layer_loss\", color='green', label=\"YMax\", ax=axes[1,0])\n",
    "    val_loss_plot.set(xlabel=\"Epochs\", ylabel=\"Loss\")\n",
    "    val_loss_plot.set_title(\"Loss vs Epochs\")\n",
    "    val_loss_plot.legend(loc=\"upper right\")\n",
    "    # Plot val_MAPE in [1,1]\n",
    "    val_mape_plot=model_data.plot(y=\"val_IndVal_Final_layer_mape\", color='blue', label=\"YNorm\", ax=axes[1,1])\n",
    "    val_mape_plot=model_data.plot(y=\"val_MaxVal_Final_layer_mape\", color='green', label=\"YMax\", ax=axes[1,1])\n",
    "    val_mape_plot.set(xlabel=\"Epochs\", ylabel=\"val_MAPE\")\n",
    "    val_mape_plot.set_title(\"Validation MAPE vs Epochs\")\n",
    "    val_mape_plot.legend(loc=\"upper right\")\n",
    "    # Subplot properties\n",
    "    plt.suptitle(\"%s Optimizer \" %(opti_name)) # Subplot title\n",
    "    plt.subplots_adjust(wspace=0.25, hspace=0.8) # Spacing details\n",
    "    plt.savefig(save_post_dir + '%s_stats.png' %(opti_name), dpi=500)\n",
    "    # plt.show(fig1)\n",
    "    plt.close(fig1)\n",
    "\n",
    "    # predictions of trained model\n",
    "    prediction=model.predict(X_pred)\n",
    "    pred = prediction[0]*prediction[1]\n",
    "\n",
    "\n",
    "    # Subplot of predictions\n",
    "    nrows=int(X_pred.shape[0]/201)\n",
    "    fig3 = plt.figure()\n",
    "    fig, axes = plt.subplots(nrows)\n",
    "    fig.set_size_inches(6,39)\n",
    "    for j in range(nrows):\n",
    "        axes[j].plot(X_n_pred[:201],\n",
    "                      pred[j*201:(j+1)*201],\n",
    "                      'r',\n",
    "                      label='Prediction')\n",
    "        axes[j].plot(X_n_pred[j*201:(j+1)*201],\n",
    "                     y_pred[j*201:(j+1)*201],\n",
    "                     'g', label='Actual')\n",
    "        axes[j].set(xlabel=\"Normalised x-coordinate\",\n",
    "                    ylabel=\"Mach Number\")\n",
    "        axes[j].set_title('L/D Ratio = %.1f with %s Optimizer' %(X_pred_max[201*j], opti_name),\n",
    "                          fontsize=11)\n",
    "        axes[j].legend(loc=\"upper right\")\n",
    "        axes[j].set_xlim((0,1))\n",
    "        axes[j].set_ylim((0,5))\n",
    "        axes[j].set_aspect(0.2)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    plt.tight_layout(pad=0.25, h_pad=1.25, w_pad=0.25, rect=None)\n",
    "    plt.savefig(save_post_dir + 'cumilative_ind_%s_predictions.png' %(opti_name), dpi=500)\n",
    "    # plt.show(fig3)\n",
    "    plt.close(fig3)\n",
    "\n",
    "    # Get Max APE for each opti\n",
    "    pred_APE = get_APE(pred, y_pred)\n",
    "    for j in range(int(X_pred.shape[0]/201)):\n",
    "        max_APE_temp = get_max_APE(pred_APE[j*201:(j+1)*201])\n",
    "        max_MAPE[i,2*j] = max_APE_temp[0]*100\n",
    "        max_MAPE[i,2*j+1] = max_APE_temp[1]\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    del(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['loss', 'IndVal_Final_layer_loss', 'MaxVal_Final_layer_loss',\n",
      "       'IndVal_Final_layer_mape', 'MaxVal_Final_layer_mape', 'val_loss',\n",
      "       'val_IndVal_Final_layer_loss', 'val_MaxVal_Final_layer_loss',\n",
      "       'val_IndVal_Final_layer_mape', 'val_MaxVal_Final_layer_mape'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(model_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_df = []\n",
    "for i in range(int(2*pred.shape[0]/201)):\n",
    "    if i%2==0:\n",
    "        column_df.append(str(X_pred_max[int(i/2)*201])+' MAPE')\n",
    "    else:\n",
    "        column_df.append(str(X_pred_max[int(i/2-1)*201])+' index')\n",
    "MAPE_df = pd.DataFrame(max_MAPE, \n",
    "                       index=opti_name_list, \n",
    "                       columns=column_df)\n",
    "\n",
    "MAPE_df.to_csv(save_post_dir + 'Cumilative_max_MAPE.csv', index=True, header=True, sep=',')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
